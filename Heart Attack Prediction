import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.externals import joblib
import time
import warnings
warnings.filterwarnings(action="ignore")

pd.set_option('display.width', 4000)
pd.set_option('display.max_column', 35)


print("\n\nDeveloped by Aditya Prakash Khare")
print('Email - aditya.khare440@gmail.com ')
print("Student/Order ID - 375035")

# Exploratory analysis
# Load the data set and do some quick exploratory analysis.
data = pd.read_csv('HeartAttack_data.csv', index_col=False)

print("\n\n\nSample HeartAttack dataset head(5) :- \n\n", data.head(5), end="\n\n\n")

print("Shape of the HeartAttack dataset data.shape = ", end="")
print(data.shape, end="\n\n\n")
# (294, 14)


# replacing custom missing values
data.replace('?', np.nan, inplace=True)
print(data, end="\n\n\n")

# changing the dtype of entire df
data = data.astype('float64')
print(data.info(), end="\n\n\n")


print("\n\n\nHeartAttack data description : \n")
print(data.describe(include='all'))


# dropping columns where the entries are less than 40 %
data = data.drop(['ca', 'thal', 'slope'], axis=1)


print("\n\n\nHeartAttack data description : \n")
print(data.describe(include='all'), end="\n\n\n")
# taking a look at modes o different columns
print(data.mode())

# handling missing values
data['chol'] = data['chol'].replace(np.nan, 235)
data['trestbps'] = data['trestbps'].replace(np.nan, 131)
data['thalach'] = data['thalach'].replace(np.nan, 138)
data['restecg'] = data['restecg'].replace(np.nan, 0)
data['fbs'] = data['fbs'].replace(np.nan, 0)
data['exang'] = data['exang'].replace(np.nan, 0)

# taking a look at cleaned data :-
print(data.describe(), end="\n\n\n")
print(data.sample(50), end="\n\n\n\n")
# a look at the columns with discrete values
print(data['restecg'].value_counts(), end="\n\n\n\n")
print(data['exang'].value_counts(), end="\n\n\n\n")
print(data['cp'].value_counts(), end="\n\n\n\n")
print(data['fbs'].value_counts(), end="\n\n\n\n")

# renaming the output column
data.rename(columns={'num': 'Target'}, inplace=True)


# a look at the target column
print("target column 1:sick , 0:healthy")
print(data['Target'].value_counts(), end="\n\n\n\n")

# graphical outlook of target column
plt.hist(data['Target'])
plt.title("Diagnosis (Sick:1 ,Healthy:0)")
plt.show()

# Next, we visualise the data using density plots to get a sense of the data distribution.
# From the outputs below, you can see the data shows a general gaussian distribution
# for the columns which have a continuous values.
data.plot(kind='density', subplots=True, layout=(4, 3), sharex=False, legend=False, fontsize=1)
plt.show()

# lets take a look at correlation matrix and see which of the columns we could work with:-
fig = plt.figure()
ax1 = fig.add_subplot(111)
cax = ax1.imshow(data.corr())
ax1.grid(True)
plt.title('Heart Attack Attributes Correlation')

# Add color bar, make sure to specify tick locations to match desired tick labels
fig.colorbar(cax, ticks=[.75, .8, .85, .90, .95, 1])
plt.show()

# after taking a look at correlation matrix , we know we cannot delete any column,
# as none are repeating(yellow in colour)

Y = data['Target'].values
X = data.drop('Target', axis=1).values

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=21)

# Baseline algorithm checking
# From the data set, we will analysis and build a model to predict if a given set of
# symptoms lead to a cancerous BrainTumor.
# This is a binary classification problem, and a few algorithms are appropriate for use.
# Since we do not know which one will perform the best at the point,
# we will do a quick test on the few algorithms to get an early indication of how each of them perform.
# We will use K-Fold cross validation for each testing.

# The following  algorithms will be used,

# 1) Classification and Regression Trees (CART),
# 2) Support Vector Machines (SVM),
# 3) Gaussian Naive Bayes (NB)
# 4) k-Nearest Neighbors (KNN).

models_list = [('CART', DecisionTreeClassifier()), ('SVM', SVC()), ('NB', GaussianNB()),
               ('KNN', KNeighborsClassifier())]

num_folds = 10

results = []
names = []

for name, model in models_list:
    kfold = KFold(n_splits=num_folds, random_state=141)
    startTime = time.time()
    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')
    endTime = time.time()
    results.append(cv_results)
    names.append(name)
    print("%s: %f (%f) (run time: %f)" % (name, cv_results.mean(), cv_results.std(), endTime-startTime))

# Performance Comparision
# ------------------------------
fig = plt.figure()
fig.suptitle('Performance Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()


# standardizing the dataset using concept of pipelines
pipelines = [("ScaledCART", Pipeline([("Scaler", StandardScaler()), ("CART", DecisionTreeClassifier())])),
             ("ScaledSVM", Pipeline([("Scaler", StandardScaler()), ("SVM", SVC())])),
             ("ScaledNB", Pipeline([("Scaler", StandardScaler()), ("NB", GaussianNB())])),
             ("ScaledKNN", Pipeline([("Scaler", StandardScaler()), ("KNN", KNeighborsClassifier())]))]

results = []
names = []

print("\n\n\nAccuracies of algorithm after scaled data set\n")

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    kfold = KFold(n_splits=num_folds, random_state=141)
    for name, model in pipelines:
        start = time.time()
        cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')
        end = time.time()
        results.append(cv_results)
        names.append(name)
        print("%s: %f (%f) (run time: %f)" % (name, cv_results.mean(), cv_results.std(), end-start))


# Performance Comparison after Scaled Data
# ----------------------------------------

fig = plt.figure()
fig.suptitle('Performance Comparison after Scaled Data')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()

print("\n\n\n")

# spot checking using PCA but not using pipeline:-

scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)
pca = PCA(n_components=5)
fit = pca.fit(X_train_scaled)
resultX = pca.transform(X_train_scaled)
print("after doing feature selection by PCA")
model = SVC()
results = cross_val_score(model, resultX, Y_train, cv=kfold)
print("Validation Score for SVM : ", results.mean())

model = DecisionTreeClassifier()
results = cross_val_score(model, resultX, Y_train, cv=kfold)
print("Validation Score for CART : ", results.mean())

model = GaussianNB()
results = cross_val_score(model, resultX, Y_train, cv=kfold)
print("Validation Score for GNB : ", results.mean())

model = KNeighborsClassifier()
results = cross_val_score(model, resultX, Y_train, cv=kfold)
print("Validation Score for KNN : ", results.mean())

print("\n\n\n")

# As SVC is doing best we will go by SVC on future data:-
model = SVC()
model.fit(resultX, Y_train)
X_test_scaled = scaler.transform(X_test)
X_test_scaled = pca.transform(X_test_scaled)
predictions = model.predict(X_test_scaled)
print("All predictions done successfully by SVM Machine Learning Algorithms")
print("\n\nAccuracy score %f" % accuracy_score(Y_test, predictions))

print("\n\n")
print("confusion_matrix = \n")
print(confusion_matrix(Y_test, predictions))

# dumping the model to a shareable file
filename = "finalized_HeartAttack_model.sav"
joblib.dump(model, filename)
print("Best Performing Model dumped successfully into a file by Joblib\n\n")


print("Developed by Aditya Prakash Khare")
print('Email - aditya.khare440@gmail.com ')
print("Student/Order ID - 375035")
